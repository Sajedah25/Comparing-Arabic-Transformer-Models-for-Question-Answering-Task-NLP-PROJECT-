# -*- coding: utf-8 -*-
"""CAMELBERT (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mZSwA_ntbwT40SA8_Jk0RhUbDZ5TUfHb

**INSTALLATION - IMPORTING**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -U transformers
!pip install evaluate
!pip install emoji
!pip install beautifulsoup4 lxml
!pip install arabic-reshaper
!pip install fuzzysearch

import collections
import re
import torch
import numpy as np
import evaluate
from transformers import TrainingArguments
from transformers import Trainer, default_data_collator
from datasets import Dataset, load_dataset
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from fuzzysearch import find_near_matches
from torch.utils.data import DataLoader

#ARABIC-SQUAD
arabic_squad = load_dataset("i0xs0/Arabic-SQuAD")
#ARCD
arcd = load_dataset("hsseinmz/arcd")
#AQAD
aqad= load_dataset("arbml/AQAD")
#TYDIQA-GOLDP-ARABIC
tydiqa= load_dataset("asas-ai/tydiqa-goldp-ar")

from transformers import AutoTokenizer, AutoModelForQuestionAnswering

tokenizer = AutoTokenizer.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-ca")
model = AutoModelForQuestionAnswering.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-ca")

"""**UNIFIYING THE STRUCTURE**

FLATTEN ARABIC SQUAD
"""

def flatten_arabic_squad_original(dataset):
    flat = []

    # Extract the actual dictionary from the HF Dataset
    sample = dataset[0]
    articles = sample["data"]

    for article in articles:
        for para in article["paragraphs"]:
            context = para["context"]
            for qa in para["qas"]:
                if len(qa["answers"]) == 0:
                    continue

                flat.append({
                    "id": qa["id"],
                    "context": context,
                    "question": qa["question"],
                    "answers": qa["answers"]
                })

    return flat

flat_squad = flatten_arabic_squad_original(arabic_squad["train"])
flat_squad = Dataset.from_list(flat_squad)

flat_squad[0]

"""ARCD AND AQAD"""

def unify_arcd_aqad(dataset):
    """Works for both ARCD and AQAD because both have the same flat structure."""
    return dataset.select_columns(["id", "context", "question", "answers"])

arcd_train = unify_arcd_aqad(arcd["train"])
arcd_valid = unify_arcd_aqad(arcd["validation"])

print(arcd_train.column_names)
print(arcd_valid.column_names)

aqad_train = unify_arcd_aqad(aqad["train"])

print(aqad_train.column_names)

"""TYDIQA"""

def unify_tydiqa(dataset):
    def convert(row):
        ans = row["answers"]

        # Use start_char as answer_start
        answer_start = ans["start_char"] if "start_char" in ans else ans["start_byte"]

        return {
            "id": row["id"],
            "context": row["passage_text"],
            "question": row["question_text"],
            "answers": {
                "text": ans["text"],
                "answer_start": answer_start
            }
        }

    return dataset.map(convert, remove_columns=dataset.column_names)

tydiqa_train = unify_tydiqa(tydiqa["train"])
tydiqa_valid = unify_tydiqa(tydiqa["validation"])

print(tydiqa_train.column_names)
print(tydiqa_valid.column_names)

print("Train size:", len(tydiqa_train))
print("Validation size:", len(tydiqa_valid))

"""**PREPROCESSING FROM THE PAPER**"""

import re
from fuzzysearch import find_near_matches

# 1. CAMELBERT CLEANING
DIAC = re.compile(r"[ًٌٍَُِّْ]")
TATWEEL = "ـ"

def camelbert_clean(text):
    if text is None:
        return ""
    # Remove diacritics
    text = re.sub(DIAC, "", text)
    # Remove tatweel
    text = text.replace(TATWEEL, "")
    # Normalize whitespace
    text = " ".join(text.split())
    return text


# 2. REALIGN ANSWER AFTER CLEANING
def realign_answer_camel(old_context, new_context, answer_text, old_start):
    cleaned_answer = camelbert_clean(answer_text)

    # Step 1: Exact match
    idx = new_context.find(cleaned_answer)
    if idx != -1:
        return cleaned_answer, idx

    # Step 2: Estimate approximate new start
    before_old = old_context[:old_start]
    before_new = camelbert_clean(before_old)

    removed = len(before_old) - len(before_new)
    est_start = max(0, old_start - removed)

    search_end = min(len(new_context), est_start + len(cleaned_answer) + 50)

    # Step 3: Fuzzy matching for robustness
    matches = find_near_matches(
        cleaned_answer,
        new_context[est_start:search_end],
        max_l_dist=min(8, len(cleaned_answer) // 2)
    )

    if matches:
        m = matches[0]
        return m.matched, est_start + m.start

    # No match found
    return cleaned_answer, -1



# 3. UNIVERSAL PREPROCESSING FUNCTION — CAMELBERT
def preprocess_camelbert_with_alignment(example, tokenizer):
    old_context = example["context"]
    new_context = camelbert_clean(old_context)
    example["context"] = new_context

    # Clean + trim question (no spans → no realignment needed)
    q = camelbert_clean(example["question"])
    q_tokens = tokenizer.tokenize(q)
    q_tokens = q_tokens[:64]  # max question length
    example["question"] = tokenizer.convert_tokens_to_string(q_tokens)

    # Handle answer formats
    answers = example["answers"]

    # FORMAT A — SQuAD list format
    if isinstance(answers, list):
        new_answers = []
        for ans in answers:
            txt = ans["text"]
            start = ans["answer_start"]
            new_text, new_start = realign_answer_camel(old_context, new_context, txt, start)
            new_answers.append({"text": new_text, "answer_start": new_start})
        example["answers"] = new_answers
        return example

    # FORMAT B — ARCD dict format
    if isinstance(answers, dict):
        new_texts = []
        new_starts = []
        for txt, start in zip(answers["text"], answers["answer_start"]):
            new_text, new_start = realign_answer_camel(old_context, new_context, txt, start)
            new_texts.append(new_text)
            new_starts.append(new_start)
        example["answers"] = {
            "text": new_texts,
            "answer_start": new_starts
        }
        return example

    return example

flat_squad_clean = flat_squad.map(
    lambda x: preprocess_camelbert_with_alignment(x, tokenizer),
    desc="Cleaning flat SQuAD (CAMeLBERT)"
)

arcd_train_clean = arcd_train.map(
    lambda x: preprocess_camelbert_with_alignment(x, tokenizer),
    desc="Cleaning ARCD Train (CAMeLBERT)"
)

arcd_valid_clean = arcd_valid.map(
    lambda x: preprocess_camelbert_with_alignment(x, tokenizer),
    desc="Cleaning ARCD Valid (CAMeLBERT)"
)

aqad_train_clean = aqad_train.map(
    lambda x: preprocess_camelbert_with_alignment(x, tokenizer),
    desc="Cleaning AQAD Train (CAMeLBERT)"
)

tydiqa_train_clean = tydiqa_train.map(
    lambda x: preprocess_camelbert_with_alignment(x, tokenizer),
    desc="Cleaning TyDiQA Train (CAMeLBERT)"
)

tydiqa_valid_clean = tydiqa_valid.map(
    lambda x: preprocess_camelbert_with_alignment(x, tokenizer),
    desc="Cleaning TyDiQA Valid (CAMeLBERT)"
)

"""**TRAINING PREPROCESSING FROM HF**"""

max_length = 384
stride = 128

def preprocess_training_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    sample_map = inputs.pop("overflow_to_sample_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        ans = answers[sample_idx]

        # CASE 1 — SQuAD format (list of dicts)
        if isinstance(ans, list):
            if len(ans) == 0:
                # No answer → label is (0, 0)
                start_positions.append(0)
                end_positions.append(0)
                continue

            ans = ans[0]  # first answer
            text = ans["text"]
            start_char = ans["answer_start"]

        # CASE 2 — ARCD / AQAD / TydiQA format (dict of lists)
        elif isinstance(ans, dict):
            if len(ans["text"]) == 0:
                # No answer → label is (0, 0)
                start_positions.append(0)
                end_positions.append(0)
                continue

            text = ans["text"][0]
            start_char = ans["answer_start"][0]

        # Compute end character
        end_char = start_char + len(text)

        sequence_ids = inputs.sequence_ids(i)

        # find context range
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while idx < len(sequence_ids) and sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # Check if answer is inside context
        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # find start token
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            # find end token
            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs

"""**VALIDATION PREPROCESSING FROM HF**"""

def preprocess_validation_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_map = inputs.pop("overflow_to_sample_mapping")
    example_ids = []

    for i in range(len(inputs["input_ids"])):
        sample_idx = sample_map[i]
        example_ids.append(examples["id"][sample_idx])

        sequence_ids = inputs.sequence_ids(i)
        offset = inputs["offset_mapping"][i]
        inputs["offset_mapping"][i] = [
            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)
        ]

    inputs["example_id"] = example_ids
    return inputs

"""**CAMELBERT ON TYDIQA**"""

train_dataset = tydiqa_train_clean.map(
    preprocess_training_examples,
    batched=True,
    remove_columns=tydiqa_train_clean.column_names,
)

valid_dataset = tydiqa_valid_clean.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=tydiqa_valid_clean.column_names,
)

args = TrainingArguments(
    output_dir="YOUR DIRACTORY",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
)

trainer.train()

trainer.save_model("YOUR DIRACTORY")
tokenizer.save_pretrained("YOUR DIRACTORY")

model_path = "YOUR DIRACTORY"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)
model.eval()

eval_set_for_model = valid_dataset.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("torch")

batch = default_data_collator(eval_set_for_model)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch = {k: v.to(device) for k, v in batch.items()}
model = model.to(device)
with torch.no_grad():
    outputs = model(**batch)

start_logits = outputs.start_logits.cpu().numpy()
end_logits = outputs.end_logits.cpu().numpy()

import collections

example_to_features = collections.defaultdict(list)
for idx, feature in enumerate(valid_dataset):
    example_to_features[feature["example_id"]].append(idx)

import numpy as np

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in tydiqa_valid_clean:
    example_id = example["id"]
    context = example["context"]
    answers = []

    for feature_index in example_to_features[example_id]:
        start_logit = start_logits[feature_index]
        end_logit = end_logits[feature_index]
        offsets = valid_dataset["offset_mapping"][feature_index]

        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
        for start_index in start_indexes:
            for end_index in end_indexes:
                # Skip answers that are not fully in the context
                if offsets[start_index] is None or offsets[end_index] is None:
                    continue
                # Skip answers with a length that is either < 0 or > max_answer_length.
                if (
                    end_index < start_index
                    or end_index - start_index + 1 > max_answer_length
                ):
                    continue

                answers.append(
                    {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                )

    best_answer = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})

metric = evaluate.load("squad")

theoretical_answers = [
    {"id": ex["id"], "answers": ex["answers"]}
    for ex in tydiqa_valid_clean
]

print(predicted_answers[1])
print(theoretical_answers[1])

metric.compute(predictions=predicted_answers, references=theoretical_answers)

"""**CAMELBERT ON ARCD**"""

arcd_train_dataset = arcd_train_clean.map(
    preprocess_training_examples,
    batched=True,
    remove_columns=arcd_train_clean.column_names,
)

arcd_test_dataset = arcd_valid_clean.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=arcd_valid_clean.column_names,
)

args = TrainingArguments(
    output_dir="YOUR DIRACTORY",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=4,
    weight_decay=0.01,
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=arcd_train_dataset,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
)

trainer.train()

trainer.save_model("YOUR DIRACTORY")
tokenizer.save_pretrained("YOUR DIRACTORY")

model_path = "YOUR DIRACTORY"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)
model.eval()

eval_set_for_model = arcd_test_dataset.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("torch")

batch = default_data_collator(eval_set_for_model)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch = {k: v.to(device) for k, v in batch.items()}
model = model.to(device)
with torch.no_grad():
    outputs = model(**batch)

start_logits = outputs.start_logits.cpu().numpy()
end_logits = outputs.end_logits.cpu().numpy()

import collections

example_to_features = collections.defaultdict(list)
for idx, feature in enumerate(arcd_test_dataset):
    example_to_features[feature["example_id"]].append(idx)

import numpy as np

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in arcd_valid_clean:
    example_id = example["id"]
    context = example["context"]
    answers = []

    for feature_index in example_to_features[example_id]:
        start_logit = start_logits[feature_index]
        end_logit = end_logits[feature_index]
        offsets = arcd_test_dataset["offset_mapping"][feature_index]

        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
        for start_index in start_indexes:
            for end_index in end_indexes:
                # Skip answers that are not fully in the context
                if offsets[start_index] is None or offsets[end_index] is None:
                    continue
                # Skip answers with a length that is either < 0 or > max_answer_length.
                if (
                    end_index < start_index
                    or end_index - start_index + 1 > max_answer_length
                ):
                    continue

                answers.append(
                    {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                )

    best_answer = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})

metric = evaluate.load("squad")

theoretical_answers = [
    {"id": ex["id"], "answers": ex["answers"]}
    for ex in arcd_valid_clean
]

print(predicted_answers[1])
print(theoretical_answers[1])

metric.compute(predictions=predicted_answers, references=theoretical_answers)

"""**CAMELBERT ON AQAD**"""

def is_valid(example):
    if example["context"] is None or example["context"] == "":
        return False
    if example["question"] is None or example["question"] == "":
        return False
    if example["answers"] is None or len(example["answers"]["text"]) == 0:
        return False
    if example["answers"]["text"][0] in [None, ""]:
        return False
    if example["answers"]["answer_start"][0] is None:
        return False
    return True

aqad_train_clean = aqad_train_clean.filter(is_valid)

len(aqad_train_clean)

aqad_train_final = aqad_train_clean.select(range(4108))
aqad_test_final  = aqad_train_clean.select(range(4108, 4108+1151))

aqad_train_dataset = aqad_train_final.map(
    preprocess_training_examples,
    batched=True,
    remove_columns=aqad_train_final.column_names,
)

aqad_test_dataset = aqad_test_final.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=aqad_test_final.column_names,
)

args = TrainingArguments(
    output_dir="YOUR DIRACTORY",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=4,
    weight_decay=0.01,
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=aqad_train_dataset,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
)

trainer.train()

trainer.save_model("YOUR DIRACTORY")
tokenizer.save_pretrained("YOUR DIRACTORY")

model_path = "YOUR DIRACTORY"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)
model.eval()

from torch.utils.data import DataLoader
import numpy as np
import torch

eval_set_for_model = aqad_test_dataset.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("torch")

loader = DataLoader(
    eval_set_for_model,
    batch_size=args.per_device_eval_batch_size, # Use the batch size defined in args
    shuffle=False,
    collate_fn=default_data_collator
)

all_start_logits = []
all_end_logits = []

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

with torch.no_grad():
    for batch in loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        all_start_logits.append(outputs.start_logits.cpu().numpy())
        all_end_logits.append(outputs.end_logits.cpu().numpy())

start_logits = np.concatenate(all_start_logits, axis=0)
end_logits = np.concatenate(all_end_logits, axis=0)

import collections

example_to_features = collections.defaultdict(list)
for idx, feature in enumerate(aqad_test_dataset):
    example_to_features[feature["example_id"]].append(idx)

import numpy as np

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in aqad_test_final:
    example_id = example["id"]
    context = example["context"]
    answers = []

    for feature_index in example_to_features[example_id]:
        start_logit = start_logits[feature_index]
        end_logit = end_logits[feature_index]
        offsets = aqad_test_dataset["offset_mapping"][feature_index]

        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
        for start_index in start_indexes:
            for end_index in end_indexes:
                # Skip answers that are not fully in the context
                if offsets[start_index] is None or offsets[end_index] is None:
                    continue
                # Skip answers with a length that is either < 0 or > max_answer_length.
                if (
                    end_index < start_index
                    or end_index - start_index + 1 > max_answer_length
                ):
                    continue

                answers.append(
                    {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                )

    best_answer = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})

metric = evaluate.load("squad")

theoretical_answers = [
    {"id": ex["id"], "answers": ex["answers"]}
    for ex in aqad_test_final
]

print(predicted_answers[1])
print(theoretical_answers[1])

metric.compute(predictions=predicted_answers, references=theoretical_answers)

"""**CAMELBERT ON ARABIC_SQuAD**"""

total = len(flat_squad_clean)
print("Total:", total)

# exact counts from the paper:
train_count = 38885
test_count  = 9459

flat_train = flat_squad_clean.select(range(0, train_count))
flat_test  = flat_squad_clean.select(range(train_count, train_count + test_count))

print(len(flat_train), len(flat_test))

squad_train_dataset = flat_train.map(
    preprocess_training_examples,
    batched=True,
    remove_columns=flat_train.column_names,
)

squad_test_dataset = flat_test.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=flat_test.column_names,
)

args = TrainingArguments(
    output_dir="YOUR DIRACTORY",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=4,
    weight_decay=0.01,
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=squad_train_dataset,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
)

trainer.train()

trainer.save_model("YOUR DIRACTORY")
tokenizer.save_pretrained("YOUR DIRACTORY")

model_path = "YOUR DIRACTORY"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)
model.eval()

from collections import defaultdict

example_to_features = defaultdict(list)

for i, feat in enumerate(squad_test_dataset):
    example_to_features[feat["example_id"]].append(i)

from torch.utils.data import DataLoader
import numpy as np
import torch

# Version for the MODEL (no offsets)
eval_dataset = squad_test_dataset.remove_columns(["example_id", "offset_mapping"])
eval_dataset.set_format("torch")

loader = DataLoader(
    eval_dataset,
    batch_size=16,
    shuffle=False,
    collate_fn=default_data_collator
)

all_start_logits = []
all_end_logits = []

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

with torch.no_grad():
    for batch in loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        # offset_mapping is NOT inside batch
        outputs = model(**batch)
        all_start_logits.append(outputs.start_logits.cpu().numpy())
        all_end_logits.append(outputs.end_logits.cpu().numpy())

start_logits = np.concatenate(all_start_logits, axis=0)
end_logits = np.concatenate(all_end_logits, axis=0)

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in flat_test:
    example_id = example["id"]
    context = example["context"]
    answers = []

    # Loop over all feature chunks for this example
    for feat_i in example_to_features[example_id]:
        start_logit = start_logits[feat_i]
        end_logit = end_logits[feat_i]
        offsets = squad_test_dataset[feat_i]["offset_mapping"]

        start_idxs = np.argsort(start_logit)[-n_best:]
        end_idxs   = np.argsort(end_logit)[-n_best:]

        for s in start_idxs:
            for e in end_idxs:
                # Skip offsets that are None (not part of context or special tokens)
                if offsets[s] is None or offsets[e] is None:
                    continue

                # Skip invalid spans
                if e < s or (e - s + 1) > max_answer_length:
                    continue

                start_char = offsets[s][0]
                end_char   = offsets[e][1]

                answers.append({
                    "text": context[start_char:end_char],
                    "logit_score": float(start_logit[s] + end_logit[e])
                })

    # If no answer found → empty prediction
    if len(answers) == 0:
        predicted_answers.append({"id": example_id, "prediction_text": ""})
        continue

    best = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best["text"]})

references = [
    {
        "id": ex["id"],
        "answers": {
            "text": [ans["text"] for ans in ex["answers"]],
            "answer_start": [ans["answer_start"] for ans in ex["answers"]]
        }
    }
    for ex in flat_test
]

print(predicted_answers[1])
print(references[1])

metric = evaluate.load("squad")
metric.compute(predictions=predicted_answers, references=references)
